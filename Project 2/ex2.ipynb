{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KHRX4NhEDHd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srrdLiikEDHe"
      },
      "source": [
        "### Problem 1\n",
        "\n",
        "Write a function that takes 1-dimensional array and a single integer (`n`) and computes a vector of sums of n-tuples (i.e. triples, quadruples) of numbers.\n",
        "\n",
        "For instance, for input\n",
        "\n",
        "```python\n",
        "np.array([1, 2, 3, 4, 5, 6])\n",
        "```\n",
        "\n",
        "and `n=3` it should return\n",
        "\n",
        "```python\n",
        "np.array([6, 15])\n",
        "```\n",
        "\n",
        "In this exercise you may assume that length of `X` is divisible by `n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqZO1kT7EDHf"
      },
      "outputs": [],
      "source": [
        "def sum_blocks(X, n):\n",
        "    return X.reshape(int(X.size/n), n).sum(1)\n",
        "\n",
        "X = np.arange(12)\n",
        "n = 4\n",
        "sum_blocks(X,n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UlNtuC6EDHf"
      },
      "source": [
        "### Problem 2 \n",
        "\n",
        "**Part I.** Write an expression that extracts numbers from the main diagonal of a square 2-dimensional array (square matrix).\n",
        "\n",
        "**Part II.** Generalize this expression so it extracts all elements with the same index along all axes (i.e. `(1,1,1)`) from multidimensional arrays. You should demonstrate that it works on an array with at least 3 axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7RqEZ7NEDHg"
      },
      "outputs": [],
      "source": [
        "# Part I\n",
        "X = np.arange(9).reshape(3, 3)\n",
        "print(X)\n",
        "Y = np.arange(min(X.shape))\n",
        "X[Y,Y]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMbGZa8-zB0N"
      },
      "outputs": [],
      "source": [
        "# Part II\n",
        "Z = np.arange(36).reshape(4, 3, 3)\n",
        "print(Z)\n",
        "G = np.arange(min(Z.shape))\n",
        "Z[tuple([G] * Z.ndim)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfg9rFxEDHg"
      },
      "source": [
        "### Problem 3\n",
        "\n",
        "You are provided with repeated measurements for 10 subjects (subjects in rows and measurements are in columns). Scale the data so minimal measurement for each subject is 0 and maximal is 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jGZZS0VEDHg"
      },
      "outputs": [],
      "source": [
        "np.random.seed(101)\n",
        "X = np.random.normal(10, 2, (10, 5))\n",
        "\n",
        "X_min = X.min(1)[:, None]\n",
        "X_max = (X - X_min).max(1)[:, None]\n",
        "X_minmax = (X - X_min) / X_max\n",
        "X_minmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6gpXHhfEDHg"
      },
      "source": [
        "### Problem 4 \n",
        "\n",
        "In the context of matrices (2-dimensional arrays) we have a natural notion of symmetry. A matrix is symmetric when it is square (has the same number of rows and columns) and elements opposite with respect to the main diagonal are the same. In other words, if $a_{ij}$ is the value in $i$-th row and $j$-th column, then in a symmetric matrix $a_{ij} = a_{ji}$.\n",
        "\n",
        "Write a function that takes one argument, an array, and returns ``True`` if the array is symmetric in the abovementioned sense and ``False`` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhHNVW7_EDHh"
      },
      "outputs": [],
      "source": [
        "def is_symmetric(X):\n",
        "  if (X.shape != X.T.shape):\n",
        "    return False\n",
        "  return (X==X.T).all()\n",
        "\n",
        "C = np.array([1, 2, 6, 2, 8, 4, 6, 4, 5]).reshape(3,3)\n",
        "is_symmetric(C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avpLXg1HDZHQ"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "X = np.eye(3)\n",
        "X[1,0] += 1e-7\n",
        "is_symmetric(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz7Tu_aLEDHh"
      },
      "source": [
        "### Problem 5\n",
        "\n",
        "You are provided with (5) repeated measurements of two variables for 10 subjects in the so-called _wide format_. The first column stores id of a subject (an integer, but can be represented as float). The next five columns store repeated measurements of the first variable and the last five columns store repeated measurements of the second variable. So there is the following structure:\n",
        "\n",
        "```python\n",
        "id  x1  x2  x3  x4  x5  y1  y2  y3  y4  y5\n",
        "id  x1  x2  x3  x4  x5  y1  y2  y3  y4  y5\n",
        "```\n",
        "\n",
        "Reshape the data so it follows _long format_. The first column should store subject id, the second column should store measurements of a first variable and the third colum should store measurements of a second variable. Thus, one subject should be represented by five rows. The structure for a single subject should be the following:\n",
        "\n",
        "```python\n",
        "id  x1  y1\n",
        "id  x2  y2\n",
        "id  x3  y3\n",
        "id  x4  y4\n",
        "id  x5  y5\n",
        "```\n",
        "\n",
        "HINT. You may want to use things like `reshape`, `flatten` and `np.concatenate`. And of course also standard indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BoqPMIrEDHi"
      },
      "outputs": [],
      "source": [
        "X = np.array([\n",
        "    [0.        , 2.56317058, 0.98026039, 1.75151219, 0.78852115, 1.99130403, 2.58547279, 3.41878433, 0.11217429, 2.50468377, 1.05568257],\n",
        "    [1.        , 4.19488948, 2.84022379, 4.83035827, 2.96672149, 1.78951895, 2.32414649, 3.60934652, 2.46478849, 4.86765982, 4.51192699],\n",
        "    [2.        , 0.91450201, 3.78570826, 2.56881192, 3.15329399, 1.5017536 , 3.18380084, 4.33284052, 4.04774677, 2.69470248, 0.47254604],\n",
        "    [3.        , 4.98860707, 1.54974995, 3.25265745, 1.28697882, 1.17695728, 4.80149117, 4.20484614, 2.66319767, 0.57097736, 4.51176382],\n",
        "    [4.        , 1.45394572, 1.13455845, 4.86483347, 3.00391594, 3.64987375, 2.01606963, 2.17335748, 2.54199603, 4.25302101, 4.23201769],\n",
        "    [5.        , 3.60347966, 0.99424997, 4.19417883, 3.30655813, 2.79292047, 0.90123628, 1.80901695, 0.65257978, 4.73270252, 2.16265222],\n",
        "    [6.        , 4.12353014, 1.57768196, 3.89417995, 0.48899792, 0.11017582, 4.45513605, 4.28963397, 1.48403315, 2.50637059, 3.88947188],\n",
        "    [7.        , 3.78277565, 3.12713444, 3.44142428, 3.61102362, 4.02108042, 2.2674287 , 2.14311441, 2.15432643, 3.32882083, 3.7768098 ],\n",
        "    [8.        , 2.31891701, 3.76819053, 2.01193816, 0.24171994, 3.39217946, 2.5362014 , 2.58117613, 1.2692732 , 1.01144995, 0.3478123 ],\n",
        "    [9.        , 1.66596028, 4.35365948, 3.97496054, 3.85366582, 0.33284131, 4.59799785, 3.72954968, 1.04616716, 0.22012838, 1.97861487]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWTa_ytmDZHS"
      },
      "outputs": [],
      "source": [
        "x_col = X[:, 1:6, None]\n",
        "y_col = X[:, 6:, None]\n",
        "id_col = X[:, 0].repeat(5).reshape(10, 5, 1)\n",
        "x_id_col = np.concatenate((id_col, x_col), 2)\n",
        "id_x_y_col = np.concatenate((x_id_col, y_col), 2)\n",
        "Y = np.concatenate(np.array_split(id_x_y_col, 10), 1).reshape(-1, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il1Bay_gDZHS"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "Y[:5]\n",
        "X[0, 1:6]\n",
        "X[0, 6:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5qSjR10EDHi"
      },
      "source": [
        "### Problem 6 \n",
        "\n",
        "Write a function that takes one integer `n` and create a `n`-by-`n` 2D array with checkerboard pattern of 0s and 1s. Like the one below:\n",
        "\n",
        "```python\n",
        "    1 0 1 0 1\n",
        "    0 1 0 1 0\n",
        "    1 0 1 0 1\n",
        "    0 1 0 1 0\n",
        "    1 0 1 0 1\n",
        "```\n",
        "\n",
        "HINT. Remember to test your function for different values of `n`. In particular pay attention to its behavior for even and odd values of `n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cFjRL3sEDHj"
      },
      "outputs": [],
      "source": [
        "def checkerboard(n):\n",
        "    symmetric = np.diag([1]*n)\n",
        "    symmetric[1::2, 1::2] = 1\n",
        "    symmetric[::2, ::2] = 1\n",
        "    return symmetric\n",
        "\n",
        "checkerboard(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VulmOraBDZHT"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "for i in range(6):\n",
        "    print(i)\n",
        "    print(checkerboard(i))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHJvW6CpEDHj"
      },
      "source": [
        "### Problem 7 \n",
        "\n",
        "You are provided with measurements of 8 variables for 20 subjects. For each subject compute sum over measurements equal or higher than one standard deviation from the mean (of a given variable). Subjects are in rows, measurements are in columns.\n",
        "\n",
        "HINT. You may want to use `np.where`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E4DvjcEEDHk"
      },
      "outputs": [],
      "source": [
        "np.random.seed(102)\n",
        "X = np.random.normal(10, 2, (20, 8))\n",
        "X_mean_sd = X.mean(0) + X.std(0)\n",
        "np.where((X >= X_mean_sd), X, 0).sum(1).reshape(20,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8fw7isEEDHk"
      },
      "source": [
        "### Problem 8 \n",
        "\n",
        "Write an expression computing Euclidean distance. It should work with `(N, M)` array (that is it should work for any number of observations and any number of dimensions).\n",
        "\n",
        "If $\\vec{u}$ and $\\vec{v}$ are position vectors of two points in $n$ dimensions then the Euclidean distance between them is:\n",
        "\n",
        "$$d(u, v) = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}$$\n",
        "\n",
        "Where $u_i$ and $v_i$ are single components (elements) of vectors $u$ and $v$.\n",
        "\n",
        "HINT. Use non-trivial broadcasting and remember that you can use negative indices with aggregating functions. For instance, `sum(-1)` sums elements along the last axis.\n",
        "\n",
        "HINT 2. Try to solve the problem step by step and analyze shapes of intermediate results all the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoTUbyLlEDHk"
      },
      "outputs": [],
      "source": [
        "# You can test your expression with this data\n",
        "np.random.seed(101)\n",
        "X = np.random.normal(10, 2, (10, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8zMOX2iEDHl"
      },
      "outputs": [],
      "source": [
        "D = np.sqrt(((X[ :, None, : ] - X)**2).sum(axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IV2K2ZJDZHV"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "from scipy.spatial.distance import cdist\n",
        "np.isclose(cdist(X, X), D).all()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKvd0wrWEDHm"
      },
      "source": [
        "### Problem 9 \n",
        "\n",
        "In linear regression we try to model a dependent variable as a linear function of one or more predictors + an intercept term (a constant). Thus, the expected value of the dependent variable for observation $i$ is:\n",
        "\n",
        "$$\\hat{y}_i = b_0 + \\sum_{k=1}^n b_kx_{i, k}$$\n",
        "\n",
        "where $b_0$ is an intercept, $b_i$ are regression coefficients and $x_i$ are predictor values ($k$ is the number of predictors).\n",
        "\n",
        "So in a simple case with only one predictor the formula is:\n",
        "\n",
        "$$\\hat{y}_i = b_0 + b_1x_i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vUlYSwNEDHm"
      },
      "source": [
        "The standard least squares approach to solving the regression problem is to minimize the sum of squares of differences between observed and predicted values of $y_i$.\n",
        "\n",
        "$$SS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "And this, assuming there is only one predictor, can be rewritten as:\n",
        "\n",
        "$$SS = \\sum_{i=1}^n (y_i - b_0 - b_1x_i)^2$$\n",
        "\n",
        "That is, in order to find optimial values of $b_0$ and $b_1$ it is enough to find values that minimize the above quantity.\n",
        "\n",
        "You are provided with two data vectors, an dependent and an independent variable, and your task is to solve the regression problem. Usually problems like that are solved in statistics using gradient-based optimization method. However, this is a calculus-free class so we will solve this problem with the gradient-free Nelder-Mead method, which we already know.\n",
        "\n",
        "You have to find optimal values of $b_0$ and $b_1$ and visualize your results by plotting points and an estimated regression line.\n",
        "\n",
        "HINT. The function you will use for optimization will have to be defined in a specific way, so it takes only one argument, an array of values of $b_0$ and $b_1$, but also uses the data, which can not be passed as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crb5KgY0EDHm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Independent Variable\n",
        "X = np.array([\n",
        "     71.3267733 ,  94.92926986, 105.84580891, 104.82694608, 100.62114327, 111.37297356, 103.8092986 ,  97.00152011,  84.40305633, 105.0855247,  \n",
        "     83.57524616,  69.57162511, 114.5138982 ,  96.01736239,  92.65684887,  97.04601589,  97.92013884,  95.18013523,  97.22035789, 103.64593584,\n",
        "     98.02858537, 108.00779543, 137.55073015, 114.98588324, 107.26593304, 101.67184601, 105.80025847, 112.14330063, 106.92253653, 117.92332618, \n",
        "    116.46776663,  97.16267619,  93.0292588 , 107.5604558 , 125.32848687,  83.86050451, 106.8823974 ,  99.27255707,  97.82226435,  96.73281089,\n",
        "    123.58500954, 116.2598948 , 102.43129446,  92.40676869, 113.99842236, 105.47568439,  93.62510574,  93.90830501, 104.04828968,  75.16803921,  \n",
        "     97.82290495,  95.84804141,  91.56282053, 110.5530338 ,  88.6349911 , 103.59486512,  94.20120727,  94.13220153,  92.66296659, 142.96712267, \n",
        "    101.54326786,  87.53482594,  75.89231019,  77.80978685,  85.18167245,  92.75149492,  96.99459832, 115.79129157,  92.78373297, 105.65022361,\n",
        "     99.54250974, 109.10578218, 107.59803889, 100.46708725,  98.47180924, 129.51941214, 101.62354304,  82.84169456,  91.25559331,  74.87991395,\n",
        "     90.26617518,  77.94892247, 115.39667757, 133.64325424, 104.15346927,  88.523857  ,  88.80022363,  80.78680748,  79.07070514,  96.2088635, \n",
        "    115.27847549,  93.99479232,  73.34922885, 131.75249797,  71.48788066, 109.15244358, 128.74896701, 118.03032288, 111.87550937,  70.33441244\n",
        "])\n",
        "# Dependent variable\n",
        "Y = np.array([\n",
        "    14.27903096, 16.61546258, 16.10194184, 12.29577335, 18.77045468, 15.46626262, 13.56596992, 15.25888117, 18.65445339, 17.04776665,\n",
        "    14.42865178,  9.23887792, 22.33239477, 17.32692439, 19.20194492, 20.25998093, 11.23740714, 17.21240194, 17.12187275,  9.93937255,\n",
        "    15.89731068, 14.44198627, 12.78448091, 18.89787197, 15.13653192, 14.99435507, 17.86008331, 16.46131696, 17.04737059, 13.04449074,\n",
        "    19.02584269, 13.63423406, 11.22885821, 14.67025108, 19.79709726,  9.00068365, 17.74243945, 13.7850024 , 20.4066461 , 16.86060833,\n",
        "    18.34538766, 19.08661455, 13.67924611, 15.67549215, 15.81905299, 13.88267195, 15.13764781, 14.01816887, 19.46548578, 13.45896184,\n",
        "    15.81797044, 14.0787446 , 18.0350213 , 10.45388383,  8.29028058, 11.80611285, 14.47776093,  8.45842599, 11.07617487, 21.35385475,\n",
        "    16.76512191,  9.34344371, 14.69495969, 17.43165653, 19.98602725, 14.80744677, 15.58922512, 16.27855612, 19.38853378, 17.15504745,\n",
        "    12.13318721, 18.45197469, 13.91884261, 16.83916346, 11.01756482, 17.61053517, 22.50121284,  9.69335793, 14.83979765, 15.99836368,\n",
        "    13.32739601, 16.04283448, 17.9643132 , 16.52579677, 13.04587418, 14.46465952,  8.50209512,  9.23143697, 13.50075286, 10.95740081,\n",
        "    15.77380485, 10.74566042, 16.09366699, 20.88305521, 13.50499864, 16.48766252, 16.26992618, 13.32069797, 22.57460779, 13.8906237\n",
        "])\n",
        "\n",
        "_ = plt.scatter(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwg_gE_QEDHn"
      },
      "outputs": [],
      "source": [
        "def f(B):\n",
        "    return ((Y - B[0] - B[1]*X)**2).sum()\n",
        "\n",
        "x0 = np.array([0, 0])\n",
        "solution = minimize(f, x0, method='Nelder-Mead')\n",
        "x_min = solution.x\n",
        "\n",
        "Y_r = x_min[0] + x_min[1]*X\n",
        "_ = plt.scatter(X, Y)\n",
        "_ = plt.plot(X, Y_r, c = 'r')\n",
        "x_min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjElsRlBEDHn"
      },
      "source": [
        "### Problem 10 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUf1qs4uEDHn"
      },
      "source": [
        "This problem is about a simple non-linear regression.\n",
        "\n",
        "You are provided with the following data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_vF2A7rEDHo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from scipy.optimize import minimize\n",
        "X = np.array([\n",
        "    11.80487872,  9.39463382,  9.02269782,  6.49385888,  6.54463248,  7.11850761,  9.11097274, 11.24275117,  8.1904627 , 12.02964658,\n",
        "    12.42769189,  9.58455582, 12.23147144,  8.88523908,  5.42737413, 12.81103526,  8.35481245, 13.04058132,  8.19823404,  5.50068988,\n",
        "    10.40467084,  6.2377319 , 11.49040207,  7.3855861 ,  9.46930248, 12.87300445,  8.60276518, 11.6571829 , 11.94830201,  8.56509908,\n",
        "    10.19782729, 12.23314398,  9.90649571, 12.08605895, 12.69007927, 11.98526123, 11.75594319, 12.3454685 ,  8.56269915, 10.21159211,\n",
        "     8.15233927,  9.36606948,  9.1511914 , 11.49074497,  9.34970393, 11.86928971,  4.56076819, 13.34957728,  8.79554299,  5.91064054,\n",
        "    11.22853392, 11.61748249, 11.18860934, 13.92359524,  9.68746577,  9.81490781, 10.22294463,  7.85662015,  8.03219378, 12.47332808,\n",
        "    10.73518905,  8.79753473, 10.28492248, 12.05423594,  8.88997501,  9.81032258, 11.77070728, 12.1133878 ,  5.09800892,  7.79480923,\n",
        "    12.2841769 ,  7.95057486,  9.99587779, 13.36959702, 11.10873242, 10.51380312, 10.49709589,  9.99703071, 10.56860019, 12.38810131\n",
        "])\n",
        "Y = np.array([\n",
        "    -12.75500297,  -5.01220704,  -4.54414575, -12.79290896, -13.12178104, -11.35676769,  -1.80420606,  -6.04961958,  -6.01315806, -10.31658831,\n",
        "     -9.43317716,  -4.33378625, -11.30492418,  -7.01010287, -19.81733255, -15.68126302,  -8.44898028, -12.58096227,  -6.77305014, -19.08417768,\n",
        "     -7.32193576, -11.91160349,  -7.33930818, -11.67726441,  -6.13730793, -19.2418866 ,  -6.58307379,  -4.94495555, -15.61870026, -10.0264871 ,\n",
        "     -7.07032256, -12.97210676,  -4.83742464, -14.01588571, -13.59102386, -11.04442092, -12.7370629 , -11.9630761 ,  -5.24350451,  -5.43502523,\n",
        "     -9.75279472,  -4.62034381,  -6.84178462, -11.40729454,  -5.03126825, -11.12215769, -27.25568249, -19.89961149,   0.77800075, -12.30588586,\n",
        "     -9.55364293, -11.4850674 ,  -6.26548475, -25.10327163,  -3.55634256,  -7.8240283 ,  -3.88466624,  -6.27472945,  -7.24964062, -12.45956284,\n",
        "     -8.34730236,  -5.91333394,  -6.89257277,  -9.60282813,  -7.48953759,  -7.7136011 , -11.0984569 ,  -8.11405538, -25.287341  ,  -8.30707325,\n",
        "    -14.41957848,  -5.40374453,  -6.14608316, -21.61191214,  -6.45270085,  -6.50826762,  -7.52309067,  -4.53476492,  -4.78923088, -11.47090574\n",
        "])\n",
        "\n",
        "_ = plt.scatter(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeDU5u3_EDHo"
      },
      "source": [
        "Clearly, the data is non-linear, so a simple linear regression is perhaps not the best approach. To see how ridiculous it is, first fit a simple linear regression to the data here (remember problem 9). Visualize the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "620TnU7lEDHo"
      },
      "outputs": [],
      "source": [
        "def f(B):\n",
        "    return ((Y - B[0] - B[1]*X)**2).sum()\n",
        "\n",
        "x0 = np.array([0, 0])\n",
        "solution = minimize(f, x0, method='Nelder-Mead')\n",
        "x_min = solution.x\n",
        "\n",
        "Y_r = x_min[0] + x_min[1]*X\n",
        "_ = plt.scatter(X, Y)\n",
        "_ = plt.plot(X, Y_r, c = 'r')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZlZjXWtEDHo"
      },
      "source": [
        "The result is clearly bad and does not capture the real trend in data. The trend looks like a quadratic function, so it may make sense to model as if it was generated by a second degree polynomial. Thus, it makes sense to model the dependent variable (for an observation $i$) as follows:\n",
        "\n",
        "$$\\hat{y}_i = b_0 + b_1x_i + b_2x_i^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQd1ED6cEDHp"
      },
      "source": [
        "Fit a regression of this form. Remember that in a regression setup data is fixed, so in order to include $x^2$ just treat as an additional variable computed based on $x$. Remember to define your function for computing sum-of-squares correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tG6dYc2EDHp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def f(B):\n",
        "    return ((Y - B[0] - B[1]*X - B[2]*(X)**2)**2).sum()\n",
        "\n",
        "x0 = np.array([1, 1, 1])\n",
        "solution = minimize(f, x0, method='Nelder-Mead')\n",
        "x_min = solution.x\n",
        "\n",
        "Z = np.sort(X)\n",
        "def f_y(Z):\n",
        "  return x_min[0] + x_min[1]*Z + x_min[2]*(Z)**2\n",
        "\n",
        "_ = plt.scatter(X, Y)\n",
        "_ = plt.plot(Z, f_y(Z), c = 'r')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqNLBHNiEDHp"
      },
      "source": [
        "Now you have the solution. However, the function you estimated is a quadratic and has one global maximum. So we would like to know it. Use the Nelder-Mead method to approximate the global optimum of the regression function (approximate the value of $x$ for which $y$ is maximum according to the model). Visualize your solution by plotting data, regression function (as a curve) and mark the maximum with a star.\n",
        "\n",
        "**NOTE.** If you know calculus you can easily compute the optimal $x$ by hand. Such answer will also be accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MorFLzhWEDHp"
      },
      "outputs": [],
      "source": [
        "Z = np.sort(X)\n",
        "def f(B):\n",
        "    return ((Y - B[0] - B[1]*X - B[2]*(X)**2)**2).sum()\n",
        "x0 = np.array([0, 0, 0])\n",
        "solution_min = minimize(f, x0, method='Nelder-Mead')\n",
        "x_min = solution_min.x\n",
        "\n",
        "def f_y(Z):\n",
        "  return x_min[0] + x_min[1]*Z + x_min[2]*(Z)**2\n",
        "def neg_f_y(Z):\n",
        "  return -f_y(Z)\n",
        "\n",
        "x0 = 0\n",
        "solution_max = minimize(neg_f_y, x0, method='Nelder-Mead')\n",
        "x_max = solution_max.x\n",
        "\n",
        "_ = plt.scatter(X, Y)\n",
        "_ = plt.scatter(x_max, f_y(x_max), marker='*', s=200, c='r')\n",
        "_ = plt.plot(Z, f_y(Z), c = 'r')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PMYvf69EDHp"
      },
      "source": [
        "### Problem 11 \n",
        "\n",
        "You are provided with the following data:\n",
        "\n",
        "* Estimated probability of success in a given task for a given person\n",
        "* A single predictor\n",
        "\n",
        "You need to build a statistical model for predicting success probabilities based on a predictor.\n",
        "\n",
        "First, build a simple linear regression model (see problem 9) to see how bad an idea this is. Visualize the model (plot points and draw a regression line)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6mPGYa9EDHq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "X = np.array([\n",
        "    11.30069031,  4.46555699,  6.03096908,  9.91458537,  9.33646538, 11.89777249,  7.63898189, 16.73755359,  9.94281534, 10.53635273,\n",
        "    13.46392499,  9.20594487, 10.19002063, 10.79760862,  7.76222999,  8.26898285, 10.01448388,  9.19152081,  9.0331354 ,  4.32967081,\n",
        "    10.16712545,  3.44987067, 10.71814191, 10.00538177,  9.58233763, 10.24090378, 11.39790444,  7.44449068,  8.21043425, 14.19752811,\n",
        "     7.00137573,  8.37768179, 10.411404  ,  9.62445534, 10.43848721, 10.5502867 ,  8.38022546, 10.5871026 , 16.26433345, 11.14463306,\n",
        "    10.40084301,  9.91487658,  7.77240286, 13.36633798, 10.61036643,  5.7237572 , 10.28716227, 11.56583254,  8.46913113, 10.1107223 ,\n",
        "     6.23388967,  9.86949102, 10.92483134, 13.5086075 , 10.59245634,  7.74682799,  7.67718942, 15.47346941, 10.1188004 ,  9.28227263\n",
        "])\n",
        "P = np.array([\n",
        "    0.98942933, 0.6048109 , 0.76856858, 0.95047371, 0.91514794, 0.98568606, 0.63423753, 0.99891577, 0.9783007 , 0.95957051,\n",
        "    0.99526575, 0.94769278, 0.8801992 , 0.98617288, 0.70508994, 0.79461539, 0.92915501, 0.95390237, 0.95487893, 0.20022629,\n",
        "    0.97754137, 0.34213832, 0.96590558, 0.96599855, 0.90128194, 0.97478947, 0.99179301, 0.64400917, 0.87007072, 0.994506  ,\n",
        "    0.72956368, 0.89479571, 0.97407274, 0.96438407, 0.981669  , 0.94624917, 0.89952038, 0.9749334 , 0.99843391, 0.98638219,\n",
        "    0.97654949, 0.94501936, 0.69771223, 0.99802086, 0.93233978, 0.78676632, 0.93932555, 0.98337093, 0.91070802, 0.93435964,\n",
        "    0.55188221, 0.95698827, 0.97399354, 0.99205869, 0.96426894, 0.73977219, 0.78307353, 0.99653148, 0.96035513, 0.87950128\n",
        "])\n",
        "\n",
        "#Transforming P\n",
        "P_log = P.copy()\n",
        "P_log[np.isreal(P_log) == True] = np.log(P_log/(1-P_log))\n",
        "\n",
        "P_exp = P_log.copy()\n",
        "P_exp[np.isreal(P_exp) == True] = np.exp(P_exp)/(1+np.exp(P_exp))\n",
        "\n",
        "_ = plt.scatter(X, P)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E8W_JzmEDHq"
      },
      "outputs": [],
      "source": [
        "# Linear regression\n",
        "def f(B):\n",
        "    return ((P - B[0] - B[1]*X)**2).sum()\n",
        "\n",
        "x0 = np.array([0, 0])\n",
        "solution = minimize(f, x0, method='Nelder-Mead')\n",
        "x_min = solution.x\n",
        "\n",
        "Y_r = x_min[0] + x_min[1]*X\n",
        "_ = plt.scatter(X, P)\n",
        "_ = plt.plot(X, Y_r, c = 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjVZmBxSEDHq"
      },
      "source": [
        "Note that solution is completely wrong, since the model can get us probabilities grater than 1 and lower than 0.\n",
        "\n",
        "The problem with linear regression is that it is based on a hidden assumption that the dependent variable can be modeled as a real variable ($y \\in \\mathbb{R}$).\n",
        "Thus, to use it, we first have to transform our dependent variable so it really become a real variable.\n",
        "\n",
        "For depedent variables within $(0, 1)$ the standard way to do it is to use the logit transformation (which is the core of the logistic regression).\n",
        "\n",
        "It is instructive to see how the logit transformation is constructed.\n",
        "\n",
        "Let us start with $p \\in (0, 1)$, which we can interpert in terms of probability. Now, notice that instead of working with $p$ we can work with so-called odds:\n",
        "\n",
        "$$\\frac{p}{1 - p} \\in (0, \\infty)$$\n",
        "\n",
        "Now we are already halfway through, since now our transformed variables can take any positvie value. However, we need to support negative values as well.\n",
        "To do this we can take natural log of odds to get:\n",
        "\n",
        "$$\\eta = \\log{\\frac{p}{1-p}} \\in (-\\infty, \\infty) = \\mathbb{R}$$\n",
        "\n",
        "This quantity is called log-odds and we can use as a dependent variable in a regression model, since it can take any value.\n",
        "\n",
        "And the crucial part is, that we can always back-transform from log-odds to get probability. For this we use the following formula:\n",
        "\n",
        "$$p = \\frac{e^{\\eta}}{1 + e^{\\eta}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZPPOJNFEDHq"
      },
      "source": [
        "So your final task is to create a linear regression model in which your dependent variable is logit of success probabilities. Visualize the results by back-transforming from log-odds to probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hG8jLKxEDHq"
      },
      "outputs": [],
      "source": [
        "def f(B):\n",
        "    return ((P_log - B[0] - B[1]*X)**2).sum()\n",
        "\n",
        "x0 = np.array([0, 0])\n",
        "solution = minimize(f, x0, method='Nelder-Mead')\n",
        "x_min = solution.x\n",
        "\n",
        "Y_r = x_min[0] + x_min[1]*X\n",
        "_ = plt.scatter(X, P_log)\n",
        "_ = plt.plot(X, Y_r, c = 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCa5Es5brTiU"
      },
      "outputs": [],
      "source": [
        "def f(B):\n",
        "    return ((P_log - B[0] - B[1]*X)**2).sum()\n",
        "\n",
        "x0 = np.array([0, 0])\n",
        "solution_min = minimize(f, x0, method='Nelder-Mead')\n",
        "x_min = solution_min.x\n",
        "\n",
        "C = np.sort(X)\n",
        "Y = x_min[0] + C * x_min[1]\n",
        "def f_exp(Z):\n",
        "  return np.exp(Z) / (1 + np.exp(Z))\n",
        "\n",
        "_ = plt.scatter(X, P)\n",
        "_ = plt.plot(C, f_exp(Y), c = 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2LtWexcEDHr"
      },
      "source": [
        "### Problem 12\n",
        "\n",
        "In this problem you will conduct a simple network analysis.\n",
        "\n",
        "One of the common representation of a network / graph (they are the same thing in this context) is with an adjacency matrix.\n",
        "\n",
        "An adjacency matrix is an $n$-by-$n$ 2-dimensional array filled with 0 and 1. It represents a network with $n$ nodes.\n",
        "\n",
        "If a node $i$ links to a node $j$, then $(a)_{ij} = 1$, that is, the cell in $i$-th row and $j$-th column in the matrix is one.\n",
        "\n",
        "![Simple graph](simple-graph.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCtPz8zsEDHr"
      },
      "source": [
        "The simple graph above can be represented with the following adjacency matrix. Only 1s are marked (0s are denoted with empty spaces), to make it more legible.\n",
        "\n",
        "```python\n",
        "   0  1  2  3  4  5  6  7\n",
        "0        1        1  1  1\n",
        "1              1        1\n",
        "2  1           1        1\n",
        "3                 1  1\n",
        "4     1  1\n",
        "5  1        1        1  1        \n",
        "6  1        1     1     1\n",
        "7  1  1  1        1  1\n",
        "```\n",
        "\n",
        "Relations in this network are symmetric so the adjacency matrix is also symmetric. In this task you will consider only symmetric networks, so you can assume that adjaceny matrices are symmetric.\n",
        "\n",
        "Adjaceny matrices are useful, because they allow us to easily compute many important quantities. One of the crucial quantities is degrees of nodes. Degree of a node is just a number of connections it has.\n",
        "\n",
        "One of the important questions one may ask about a structure of a network is the degree to which nodes with many connections tend to connect to other nodes with many connections. In other words, we may ask about correlation between degrees of adjacent nodes. The level of correlation between nodes' degrees is often called degree assortativity. High degree assortativity means that the correlation is positive and high. High degree disassortativity means that the correlation is high but negative.\n",
        "\n",
        "Your task is to estimate degree assortativity of the above network. More formally, degree assortativity is defined just as the standard linear (Pearson) correlation between degrees of adjacent nodes (you can compute linear correlation in Numpy with `np.corrcoef` function).\n",
        "\n",
        "Thus, you have to compute node degrees and match nodes that are connected and then compute correlation between nodes' degrees.\n",
        "\n",
        "HINT. You may find `np.argwhere` or `np.nonzero` useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekwnAddXEDHr"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "# Adjacency matrix\n",
        "A = np.array([\n",
        "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
        "    [0, 0, 0, 0, 1, 0, 0, 1],\n",
        "    [1, 0, 0, 0, 1, 0, 0, 1],\n",
        "    [0, 0, 0, 0, 0, 1, 1, 0],\n",
        "    [0, 1, 1, 0, 0, 0, 0, 0],\n",
        "    [1, 0, 0, 1, 0, 0, 1, 1],\n",
        "    [1, 0, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 1, 1, 0, 0, 1, 1, 0]\n",
        "])\n",
        "G = nx.DiGraph(A)\n",
        "nx.degree_assortativity_coefficient(G)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAWGe_T5EDHr"
      },
      "source": [
        "### Problem 13\n",
        "\n",
        "In this problem you will implement a simple (and a little naive) version of one of the most famous data mining algorithms, the $k$-means method for clustering.\n",
        "\n",
        "It can be used to determine the best partition of a data set into $k$ clusters. It has many drawbacks, but is quite easy to understand and implement.\n",
        "\n",
        "The idea is simple, we start with a data set, for instance like the one below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6nFJ9OAEDHr"
      },
      "outputs": [],
      "source": [
        "X = np.array([\n",
        "    [  1.54013846,  -1.73480383], [ -0.7578704 ,  -9.02329174], [  3.99966927,   1.2903767 ], [  3.38748409,  -1.17913621],  [  0.94694359,   2.15517046],\n",
        "    [  0.21892809,  -9.37581673], [ -0.69688778,  -9.61306308], [  5.07880828,   4.04134082], [  3.31602946,   3.42717326],  [ -2.43318757,  -9.33589493],\n",
        "    [  2.5201136 ,  -0.85355754], [  1.35061182,   1.62536925], [ -1.82231311,  -9.25508281], [  1.7957845 ,  -1.38386834],  [  3.53762083,  -1.9007099 ],\n",
        "    [  3.13206565,   1.55040925], [  3.01074114,   3.30928569], [ -1.2744919 , -10.14521343], [  3.40652175,  -0.47005596],  [  2.55660158,   0.22041301],\n",
        "    [  5.0867281 ,   2.05859475], [  2.55836562,   0.33629538], [ -0.24443748,  -9.57991887], [ -1.57628118, -11.46947898],  [  0.73759311,  -1.16484683],\n",
        "    [ -2.33708102,  -9.27378953], [  1.45120651,   1.69667193], [  2.49457064,  -2.23585975], [  1.65691518,  -1.75730908],  [  2.84999915,   0.87926069],\n",
        "    [  2.53708149,   1.77704811], [ -0.50700114,  -9.95119892], [  2.14172674,  -3.25574622], [  1.89918648,  -2.22236133],  [  2.79788283,   1.14352552],\n",
        "    [  3.58266963,   0.99457642], [  2.42886196,   1.35681208], [  2.69611178,   1.78606675], [  0.42879479, -10.37923707],  [ -2.45597214,  -8.97640646],\n",
        "    [  3.03087308,   1.98434698], [  3.02553479,   1.25145006], [ -1.12354458, -10.97524818], [  3.95443297,   2.48249396],  [  0.0883085 ,  -9.41739555],\n",
        "    [  2.15040152,  -2.76723188], [  3.72509838,   1.04780492], [ -1.45002149, -10.97842342], [  1.1139807 ,   1.04561181],  [ -2.55844247,  -9.64338813],\n",
        "    [ -1.32790974, -11.36262993], [  4.04149896,   2.70726759], [  0.86286505,  -1.61162284], [ -1.1317768 , -10.95005366],  [  1.66773376,   0.37295477],\n",
        "    [  2.6127873 ,  -3.78673996], [  2.38857752,  -3.23900261], [ -1.31709344, -10.2907085 ], [  0.24376489,  -8.30104475],  [  2.55419575,   3.13156607],\n",
        "    [ -2.52133241, -10.10645797], [ -0.61408923, -10.64233172], [ -1.80452288,  -8.8626724 ], [  3.60463791,   3.1747894 ],  [  2.83515529,  -1.57470632],\n",
        "    [  3.93077695,   2.45929838], [  4.30162004,   0.59959566], [  2.55340712,  -2.40224766], [  2.96801887,  -1.98048339],  [  2.47699877,  -1.64089422],\n",
        "    [ -2.15831728, -11.32256099], [  1.23352199,  -1.51799339], [  3.35296166,   1.62371625], [ -0.76628478, -11.31284963],  [  2.55004295,   0.04634099],\n",
        "    [  1.86623322,  -0.95250211], [ -0.16776777,  -8.93035932], [  3.19083292,  -0.82858944], [  0.66112056,  -8.98715858],  [  3.73612171,  -0.05031094],\n",
        "    [  3.5618122 ,  -1.1674852 ], [  2.06375432,  -2.29083452], [  2.18501784,   0.27811955], [ -0.83659761, -10.30390896],  [  5.66603335,   1.92377596],\n",
        "    [  2.89747932,  -0.09387543], [ -1.72272881,  -9.14770728], [  2.40786036,  -3.16566455], [  1.52349384,  -1.69622841],  [ -0.47313932,  -8.12175999],\n",
        "    [  3.21183892,   1.81193441], [  2.97426841,   0.39649209], [  2.41273314,   3.73415136], [  2.87650204,   2.13450449],  [ -0.8484656 , -10.20224016],\n",
        "    [  4.1357904 ,   2.38323296], [  3.18237521,  -0.24141073], [  2.01997044,  -0.52005336], [ -0.40337174, -10.41182613],  [ -1.17852069,  -9.51804849]\n",
        "])\n",
        "\n",
        "_ = plt.scatter(X[:, 0], X[:, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSeQsPphEDHs"
      },
      "source": [
        "This is a relatively simple case, since we can already guess that there are two clusters. However, it is very good for educational purposes as it makes it trivial to distiguish between correct and incorrect solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJL-jKn4EDHs"
      },
      "source": [
        "The algorithm you have to implement is defined as follows:\n",
        "\n",
        "1. Pick $k$, the number of clusters you are looking for.\n",
        "2. Initialize cluster centers (centroids) randomly. They have labels $0, 1, \\ldots, k-1$.\n",
        "3. Assign labels to points. Each point is assigned a label according of the closest centroid.\n",
        "4. Move centroids to the average position over all points with the corresponding label.\n",
        "5. Repeat 3 and 4 until centroid positions does not change anymore.\n",
        "\n",
        "So this is an iterative algorithm in which you have to repeat some steps until the results you get do not change anymore (so they converged to an optimal solution).\n",
        "\n",
        "You will need to compute Euclidean distances between points, so you may want to solve Problem 8 first.\n",
        "\n",
        "Implement the algorithm and run on the data you are provided with. Visualize you solution. The plot should draw both points and the centroids. Groups should have different colors.\n",
        "\n",
        "Additionally, you may plot current solutions every few steps to really see how the algorithm works. This can be really interesting.\n",
        "\n",
        "Remember to run your code multiple times with different initial centroid positions to make sure it works correctly. It may sometimes happen that you may start with bad initial positions and get a degenarate solution in which all points are assigned to one cluster. When does it happen? Can you initialize random centroids in a way that would prevent this from ever happening?\n",
        "\n",
        "HINT. In this problem you will have to use an explicit loop. It can be either a `for` or a `while` loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAKdUOP-EDHs"
      },
      "outputs": [],
      "source": [
        "C1 = np.array([0 , 1], dtype=float)\n",
        "C2 = np.array([0 , 0], dtype=float)\n",
        "C = np.array([C1, C2])\n",
        "C_before = np.empty((2,2), float)\n",
        "\n",
        "while (C_before==C).all() == False:\n",
        "  C_before = C.copy()\n",
        "  D = np.sqrt(((X[:, None, : ] - C)**2).sum(axis=-1))\n",
        "  I = np.argmin(D, axis=1)\n",
        "\n",
        "  C[0] = X[I == 0].mean(0)\n",
        "  C[1] = X[I == 1].mean(0)\n",
        "\n",
        "_ = plt.scatter(X[I == 0, 0], X[I == 0, 1], label='cluster 1', c='red')\n",
        "_ = plt.scatter(X[I == 1, 0], X[I == 1, 1], label='cluster 2', c='blue')\n",
        "_ = plt.scatter(C[:, 0], C[:, 1], c='orange', s=200)\n",
        "_ = plt.legend(loc='lower right')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ex2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}